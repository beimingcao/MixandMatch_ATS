{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from IPython.display import Audio\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from nnmnkwii.preprocessing.f0 import interp1d\n",
    "from nnmnkwii.util import apply_delta_windows\n",
    "import pandas as pd\n",
    "\n",
    "import pyworld\n",
    "import pysptk\n",
    "import nnmnkwii\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_features(wav_path, fs, frame_period, order):\n",
    "  \n",
    "    x, sr = librosa.load(wav_path, sr = fs)\n",
    "    x = x.astype(np.float64)\n",
    "    f0, timeaxis = pyworld.dio(x, fs, frame_period=frame_period)\n",
    "    f0 = pyworld.stonemask(x, f0, timeaxis, fs)\n",
    "    spectrogram = pyworld.cheaptrick(x, f0, timeaxis, fs)\n",
    "\n",
    "    mgc = pysptk.sp2mc(spectrogram, order=order,\n",
    "                       alpha=pysptk.util.mcepalpha(fs))\n",
    "\n",
    "\n",
    "    mgc_delta = apply_delta_windows(mgc, windows)\n",
    "    \n",
    "    return mgc, mgc_delta\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error')\n",
    "    plt.plot(history.epoch, np.array(history.history['mean_absolute_error']), label = 'Train Loss')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_mean_absolute_error']), label = 'Valid Loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DN004', 'DN011', 'DN006', 'DN008', 'DN009', 'DN005', 'DN007', 'DN010']\n"
     ]
    }
   ],
   "source": [
    "data_folder = '/home/beiming/Desktop/Parsed_data'\n",
    "group_name = 'SDTL' # ENF or ENM or SDTL\n",
    "group_folder = os.path.join(data_folder, group_name)\n",
    "subject_list = os.listdir(group_folder) \n",
    "print(subject_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgc_dim = 40\n",
    "lf0_dim = 1\n",
    "vuv_dim = 1\n",
    "bap_dim = 1\n",
    "\n",
    "fs = 16000\n",
    "\n",
    "frame_period = 5\n",
    "hop_length = 80\n",
    "fftlen = 1024\n",
    "alpha = 0.41\n",
    "\n",
    "order = 39\n",
    "frame_period = 5\n",
    "windows = [\n",
    "    (0, 0, np.array([1.0])),\n",
    "    (1, 1, np.array([-0.5, 0.0, 0.5])),\n",
    "    (1, 1, np.array([1.0, -2.0, 1.0])),\n",
    "]\n",
    "\n",
    "\n",
    "train_index = [i for j in (range(0, 122), range(132, 254)) for i in j]\n",
    "valid_index = [i for j in (range(122, 127), range(254, 259)) for i in j]\n",
    "test_index = [i for j in (range(127, 132), range(259, 264)) for i in j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_Target_Sub = 'DN008'\n",
    "F_Train_Sub = ['DN010', 'DN006', 'DN007']\n",
    "\n",
    "M_Target_Sub = 'DN011'\n",
    "M_Train_Sub = ['DN007','DN005', 'DN010']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Testing_Gender = 'F'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Testing_Gender == 'F':\n",
    "    Target_Sub = F_Target_Sub\n",
    "    Train_Sub = F_Train_Sub\n",
    "else:\n",
    "    Target_Sub = M_Target_Sub\n",
    "    Train_Sub = M_Train_Sub    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub_folder = os.path.join(group_folder, Target_Sub)\n",
    "\n",
    "Target_WAV_path_list = os.path.join(data_sub_folder, '*' + '.wav')\n",
    "Target_WAV_path_list = glob.glob(Target_WAV_path_list)\n",
    "Target_WAV_path_list.sort()\n",
    "\n",
    "Target_EMA_path_list = os.path.join(data_sub_folder, '*' + '.MV8')\n",
    "Target_EMA_path_list = glob.glob(Target_EMA_path_list)\n",
    "Target_EMA_path_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_sub1_folder = os.path.join(group_folder, Train_Sub[0])\n",
    "Train1_WAV_path_list = os.path.join(Train_sub1_folder, '*' + '.wav')\n",
    "Train1_WAV_path_list = glob.glob(Train1_WAV_path_list)\n",
    "Train1_WAV_path_list.sort()\n",
    "Train1_EMA_path_list = os.path.join(Train_sub1_folder, '*' + '.MV8')\n",
    "Train1_EMA_path_list = glob.glob(Train1_EMA_path_list)\n",
    "Train1_EMA_path_list.sort()\n",
    "\n",
    "Train_sub2_folder = os.path.join(group_folder, Train_Sub[1])\n",
    "Train2_WAV_path_list = os.path.join(Train_sub2_folder, '*' + '.wav')\n",
    "Train2_WAV_path_list = glob.glob(Train2_WAV_path_list)\n",
    "Train2_WAV_path_list.sort()\n",
    "Train2_EMA_path_list = os.path.join(Train_sub2_folder, '*' + '.MV8')\n",
    "Train2_EMA_path_list = glob.glob(Train2_EMA_path_list)\n",
    "Train2_EMA_path_list.sort()\n",
    "\n",
    "Train_sub3_folder = os.path.join(group_folder, Train_Sub[2])\n",
    "Train3_WAV_path_list = os.path.join(Train_sub3_folder, '*' + '.wav')\n",
    "Train3_WAV_path_list = glob.glob(Train3_WAV_path_list)\n",
    "Train3_WAV_path_list.sort()\n",
    "Train3_EMA_path_list = os.path.join(Train_sub3_folder, '*' + '.MV8')\n",
    "Train3_EMA_path_list = glob.glob(Train3_EMA_path_list)\n",
    "Train3_EMA_path_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beiming/.local/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "Valid_MV = {}\n",
    "Valid_WAV = {}\n",
    "\n",
    "index = 0\n",
    "\n",
    "for i in valid_index:\n",
    "  \n",
    "  MV = np.genfromtxt(Target_EMA_path_list[i], skip_header=1, skip_footer=1)\n",
    "  \n",
    "  WAV, WAV_delta = collect_features(Target_WAV_path_list[i], fs,  frame_period, order)\n",
    "\n",
    "  scale_ratio = WAV.shape[0] / MV.shape[0]\n",
    "\n",
    "\n",
    "  MV_align = np.empty([WAV.shape[0], MV.shape[1]])\n",
    "\n",
    "  for j in range(MV.shape[1]):\n",
    "\n",
    "    MV_align[:,j] = ndimage.zoom(MV[:,j], scale_ratio)\n",
    "    \n",
    "  MV_delta = apply_delta_windows(MV_align, windows)\n",
    "  \n",
    "  \n",
    "  Valid_MV[index] = MV_delta\n",
    "  Valid_WAV[index] = WAV_delta\n",
    "  \n",
    "  index = index + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_MV = {}\n",
    "Test_WAV = {}\n",
    "\n",
    "index = 0\n",
    "\n",
    "for i in test_index:\n",
    "  \n",
    "  MV = np.genfromtxt(Target_EMA_path_list[i], skip_header=1, skip_footer=1)\n",
    "  \n",
    "  WAV, WAV_delta = collect_features(Target_WAV_path_list[i], fs,  frame_period, order)\n",
    "\n",
    "  scale_ratio = WAV.shape[0] / MV.shape[0]\n",
    "\n",
    "\n",
    "  MV_align = np.empty([WAV.shape[0], MV.shape[1]])\n",
    "\n",
    "  for j in range(MV.shape[1]):\n",
    "\n",
    "    MV_align[:,j] = ndimage.zoom(MV[:,j], scale_ratio)\n",
    "    \n",
    "  MV_delta = apply_delta_windows(MV_align, windows)\n",
    "  \n",
    "  \n",
    "  Test_MV[index] = MV_delta\n",
    "  Test_WAV[index] = WAV_delta\n",
    "  \n",
    "  index = index + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_MV_1 = {}\n",
    "Train_WAV_1 = {}\n",
    "\n",
    "index = 0\n",
    "\n",
    "for i in train_index:\n",
    "  \n",
    "  MV = np.genfromtxt(Train1_EMA_path_list[i], skip_header=1, skip_footer=1)\n",
    "  \n",
    "  WAV, WAV_delta = collect_features(Train1_WAV_path_list[i], fs,  frame_period, order)\n",
    "\n",
    "  scale_ratio = WAV.shape[0] / MV.shape[0]\n",
    "\n",
    "\n",
    "  MV_align = np.empty([WAV.shape[0], MV.shape[1]])\n",
    "\n",
    "  for j in range(MV.shape[1]):\n",
    "\n",
    "    MV_align[:,j] = ndimage.zoom(MV[:,j], scale_ratio)\n",
    "    \n",
    "  MV_delta = apply_delta_windows(MV_align, windows)\n",
    "  \n",
    "  \n",
    "  Train_MV_1[index] = MV_delta\n",
    "  Train_WAV_1[index] = WAV_delta\n",
    "  \n",
    "  index = index + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_MV_2 = {}\n",
    "Train_WAV_2 = {}\n",
    "\n",
    "index = 0\n",
    "\n",
    "for i in train_index:\n",
    "  \n",
    "  MV = np.genfromtxt(Train2_EMA_path_list[i], skip_header=1, skip_footer=1)\n",
    "  \n",
    "  WAV, WAV_delta = collect_features(Train2_WAV_path_list[i], fs,  frame_period, order)\n",
    "\n",
    "  scale_ratio = WAV.shape[0] / MV.shape[0]\n",
    "\n",
    "\n",
    "  MV_align = np.empty([WAV.shape[0], MV.shape[1]])\n",
    "\n",
    "  for j in range(MV.shape[1]):\n",
    "\n",
    "    MV_align[:,j] = ndimage.zoom(MV[:,j], scale_ratio)\n",
    "    \n",
    "  MV_delta = apply_delta_windows(MV_align, windows)\n",
    "  \n",
    "  \n",
    "  Train_MV_2[index] = MV_delta\n",
    "  Train_WAV_2[index] = WAV_delta\n",
    "  \n",
    "  index = index + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_MV_3 = {}\n",
    "Train_WAV_3 = {}\n",
    "\n",
    "index = 0\n",
    "\n",
    "for i in train_index:\n",
    "  \n",
    "  MV = np.genfromtxt(Train3_EMA_path_list[i], skip_header=1, skip_footer=1)\n",
    "  \n",
    "  WAV, WAV_delta = collect_features(Train3_WAV_path_list[i], fs,  frame_period, order)\n",
    "\n",
    "  scale_ratio = WAV.shape[0] / MV.shape[0]\n",
    "\n",
    "\n",
    "  MV_align = np.empty([WAV.shape[0], MV.shape[1]])\n",
    "\n",
    "  for j in range(MV.shape[1]):\n",
    "\n",
    "    MV_align[:,j] = ndimage.zoom(MV[:,j], scale_ratio)\n",
    "    \n",
    "  MV_delta = apply_delta_windows(MV_align, windows)\n",
    "  \n",
    "  \n",
    "  Train_MV_3[index] = MV_delta\n",
    "  Train_WAV_3[index] = WAV_delta\n",
    "  \n",
    "  index = index + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_split(alpha_init, reduction):\n",
    "    \n",
    "    a_1 = [alpha_init[0]*reduction, alpha_init[1], alpha_init[2]]\n",
    "    a_2 = [alpha_init[0], alpha_init[1]*reduction, alpha_init[2]]\n",
    "    a_3 = [alpha_init[0], alpha_init[1], alpha_init[2]*reduction]\n",
    "    \n",
    "    alpha_1 = a_1/sum(a_1)\n",
    "    alpha_2 = a_2/sum(a_2)\n",
    "    alpha_3 = a_3/sum(a_3)\n",
    "    \n",
    "    node_1 = np.floor(244 * alpha_1).astype(int)\n",
    "    node_2 = np.floor(244 * alpha_2).astype(int)\n",
    "    node_3 = np.floor(244 * alpha_3).astype(int)\n",
    "\n",
    "    nodes = np.zeros([3,3], int)\n",
    "    alphas = np.zeros([3,3], float)\n",
    "\n",
    "    nodes[0,:] = node_1\n",
    "    nodes[1,:] = node_2\n",
    "    nodes[2,:] = node_3\n",
    "    \n",
    "    alphas[0,:] = alpha_1\n",
    "    alphas[1,:] = alpha_2\n",
    "    alphas[2,:] = alpha_3\n",
    "   \n",
    "    return nodes, alphas\n",
    "\n",
    "def shuffle_list(input_wav_list, input_mv_list):\n",
    "    \n",
    "    shuffled_wav_list = {}\n",
    "    shuffled_mv_list = {}\n",
    "    \n",
    "    keys = np.arange(len(input_wav_list))\n",
    "    np.random.shuffle(keys)    \n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    for key in keys:\n",
    "\n",
    "        shuffled_wav_list[i] = input_wav_list[key] \n",
    "        shuffled_mv_list[i] = input_mv_list[key] \n",
    "        \n",
    "        i = i + 1\n",
    "        \n",
    "    return shuffled_wav_list, shuffled_mv_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error')\n",
    "    plt.plot(history.epoch, np.array(history.history['mean_absolute_error']), label = 'Train Loss')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_mean_absolute_error']), label = 'Valid Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch % 100 == 0: print(' ')\n",
    "        print('.', end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_42 (Dense)             (None, 512)               12800     \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 120)               61560     \n",
      "=================================================================\n",
      "Total params: 1,124,984\n",
      "Trainable params: 1,124,984\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(512, activation = tf.nn.relu, input_shape = (24,)),\n",
    "        layers.Dense(512, activation = tf.nn.relu),\n",
    "        layers.Dense(512, activation = tf.nn.relu),\n",
    "        layers.Dense(512, activation = tf.nn.relu),\n",
    "        layers.Dense(512, activation = tf.nn.relu),\n",
    "        layers.Dense(120)\n",
    "    ])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.SGD(0.003)\n",
    "    model.compile(loss = 'mse', optimizer = optimizer, metrics = ['mae', 'mse'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.333 0.333 0.333]\n",
      "[[0.2 0.4 0.4]\n",
      " [0.4 0.2 0.4]\n",
      " [0.4 0.4 0.2]]\n",
      "[[48 97 97]\n",
      " [97 48 97]\n",
      " [97 97 48]]\n"
     ]
    }
   ],
   "source": [
    "alpha_init = np.array([0.333, 0.333, 0.333])\n",
    "reduction = 0.5\n",
    "nodes, alphas = node_split(alpha_init, reduction)\n",
    "\n",
    "print(alpha_init)\n",
    "print(alphas)\n",
    "print(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "Valid_MV_block = np.concatenate([Valid_MV[x] for x in Valid_MV], 0).astype(np.float32)\n",
    "Valid_WAV_block = np.concatenate([Valid_WAV[x] for x in Valid_WAV], 0)\n",
    "\n",
    "Test_MV_block = np.concatenate([Test_MV[x] for x in Test_MV], 0).astype(np.float32)\n",
    "Test_WAV_block = np.concatenate([Test_WAV[x] for x in Test_WAV], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      ". . [0.3354039 0.3350686]\n",
      " \n",
      ".  \n",
      ". . [0.32748094 0.32624474]\n",
      " \n",
      ".  \n",
      ". . [0.32067353 0.32114124]\n",
      " \n",
      ". "
     ]
    }
   ],
   "source": [
    "l = 0\n",
    "\n",
    "h\n",
    "Val_loss_forward = np.zeros(3, float)\n",
    "Train_loss_forward = np.zeros(3, float)\n",
    "\n",
    "Val_loss_backward = np.zeros(3, float)\n",
    "Train_loss_backward = np.zeros(3, float)\n",
    "\n",
    "for node in nodes:\n",
    "    \n",
    "    Train_MV_1_picked = {}\n",
    "    Train_WAV_1_picked = {}\n",
    "\n",
    "    Train_MV_2_picked = {}\n",
    "    Train_WAV_2_picked = {}\n",
    "\n",
    "    Train_MV_3_picked = {}\n",
    "    Train_WAV_3_picked = {}\n",
    "#################################### Forward ######################    \n",
    "    \n",
    "    for i in range(node[0]):        \n",
    "        Train_MV_1_picked[i] = Train_MV_1[i]\n",
    "        Train_WAV_1_picked[i] = Train_WAV_1[i]\n",
    "        \n",
    "    for i in range(node[1]): \n",
    "        \n",
    "        Train_MV_2_picked[i] = Train_MV_2[i]\n",
    "        Train_WAV_2_picked[i] = Train_WAV_2[i]\n",
    "        \n",
    "    for i in range(node[2]):\n",
    "        Train_MV_3_picked[i] = Train_MV_3[i]\n",
    "        Train_WAV_3_picked[i] = Train_WAV_3[i]\n",
    "        \n",
    "    Train_MV_picked = list(Train_MV_1_picked.values()) + list(Train_MV_2_picked.values()) + list(Train_MV_3_picked.values()) \n",
    "    Train_WAV_picked = list(Train_WAV_1_picked.values()) + list(Train_WAV_2_picked.values()) + list(Train_WAV_3_picked.values()) \n",
    "    \n",
    "    Train_WAV_picked_shuffle, Train_MV_picked_shuffle = shuffle_list(Train_WAV_picked, Train_MV_picked)\n",
    "    \n",
    "    Train_MV_block = np.concatenate([Train_MV_picked_shuffle[x] for x in Train_MV_picked_shuffle], 0).astype(np.float32)\n",
    "    Train_WAV_block = np.concatenate([Train_WAV_picked_shuffle[x] for x in Train_WAV_picked_shuffle], 0)\n",
    "\n",
    "    history = model.fit(Train_MV_block, Train_WAV_block, batch_size = 128, epochs = 2, validation_data = (Valid_MV_block, Valid_WAV_block), verbose = 0, callbacks = [PrintDot()])\n",
    "    val_loss_f = np.array(history.history['val_mean_absolute_error'])\n",
    "    train_loss_f = np.array(history.history['mean_absolute_error'])\n",
    "    \n",
    "    print(val_loss_f)\n",
    "    \n",
    "    Val_loss_forward[l] = val_loss_f[0]\n",
    "    Train_loss_forward[l] = train_loss_f[0]\n",
    "    \n",
    "######################################## Backward ###########################    \n",
    "    \n",
    "    for i in range(node[2]):        \n",
    "        Train_MV_1_picked[i] = Train_MV_1[i]\n",
    "        Train_WAV_1_picked[i] = Train_WAV_1[i]\n",
    "        \n",
    "    for i in range(node[1]): \n",
    "        \n",
    "        Train_MV_2_picked[i] = Train_MV_2[i]\n",
    "        Train_WAV_2_picked[i] = Train_WAV_2[i]\n",
    "        \n",
    "    for i in range(node[0]):\n",
    "        Train_MV_3_picked[i] = Train_MV_3[i]\n",
    "        Train_WAV_3_picked[i] = Train_WAV_3[i]\n",
    "        \n",
    "    Train_MV_picked = list(Train_MV_1_picked.values()) + list(Train_MV_2_picked.values()) + list(Train_MV_3_picked.values()) \n",
    "    Train_WAV_picked = list(Train_WAV_1_picked.values()) + list(Train_WAV_2_picked.values()) + list(Train_WAV_3_picked.values()) \n",
    "    \n",
    "    Train_WAV_picked_shuffle, Train_MV_picked_shuffle = shuffle_list(Train_WAV_picked, Train_MV_picked)\n",
    "    \n",
    "    Train_MV_block = np.concatenate([Train_MV_picked_shuffle[x] for x in Train_MV_picked_shuffle], 0).astype(np.float32)\n",
    "    Train_WAV_block = np.concatenate([Train_WAV_picked_shuffle[x] for x in Train_WAV_picked_shuffle], 0)\n",
    "\n",
    "    history = model.fit(Train_MV_block, Train_WAV_block, batch_size = 128, epochs = 1, validation_data = (Valid_MV_block, Valid_WAV_block), verbose = 0, callbacks = [PrintDot()])\n",
    "    val_loss_b = np.array(history.history['val_mean_absolute_error'])\n",
    "    train_loss_b = np.array(history.history['mean_absolute_error'])\n",
    "    \n",
    "    Val_loss_backward[l] = val_loss_b[0]\n",
    "    Train_loss_backward[l] = train_loss_b[0]\n",
    "    \n",
    "    l = l+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_nodes(nodes, h):\n",
    "    \n",
    "    l = 0\n",
    "    \n",
    "    Val_loss_forward = np.zeros([3,1], float)\n",
    "    Train_loss_forward = np.zeros([3,1], float)\n",
    "\n",
    "    Val_loss_backward = np.zeros([3,1], float)\n",
    "    Train_loss_backward = np.zeros([3,1], float)\n",
    "\n",
    "    for node in nodes:\n",
    "\n",
    "        Train_MV_1_picked = {}\n",
    "        Train_WAV_1_picked = {}\n",
    "\n",
    "        Train_MV_2_picked = {}\n",
    "        Train_WAV_2_picked = {}\n",
    "\n",
    "        Train_MV_3_picked = {}\n",
    "        Train_WAV_3_picked = {}\n",
    "    #################################### Forward ######################    \n",
    "\n",
    "        for i in range(node[0]):        \n",
    "            Train_MV_1_picked[i] = Train_MV_1[i]\n",
    "            Train_WAV_1_picked[i] = Train_WAV_1[i]\n",
    "\n",
    "        for i in range(node[1]): \n",
    "\n",
    "            Train_MV_2_picked[i] = Train_MV_2[i]\n",
    "            Train_WAV_2_picked[i] = Train_WAV_2[i]\n",
    "\n",
    "        for i in range(node[2]):\n",
    "            Train_MV_3_picked[i] = Train_MV_3[i]\n",
    "            Train_WAV_3_picked[i] = Train_WAV_3[i]\n",
    "\n",
    "        Train_MV_picked = list(Train_MV_1_picked.values()) + list(Train_MV_2_picked.values()) + list(Train_MV_3_picked.values()) \n",
    "        Train_WAV_picked = list(Train_WAV_1_picked.values()) + list(Train_WAV_2_picked.values()) + list(Train_WAV_3_picked.values()) \n",
    "\n",
    "        Train_WAV_picked_shuffle, Train_MV_picked_shuffle = shuffle_list(Train_WAV_picked, Train_MV_picked)\n",
    "\n",
    "        Train_MV_block = np.concatenate([Train_MV_picked_shuffle[x] for x in Train_MV_picked_shuffle], 0).astype(np.float32)\n",
    "        Train_WAV_block = np.concatenate([Train_WAV_picked_shuffle[x] for x in Train_WAV_picked_shuffle], 0)\n",
    "\n",
    "        history = model.fit(Train_MV_block, Train_WAV_block, batch_size = 128, epochs = h, validation_data = (Valid_MV_block, Valid_WAV_block), verbose = 0, callbacks = [PrintDot()])\n",
    "        val_loss_f = np.array(history.history['val_mean_absolute_error'])\n",
    "        train_loss_f = np.array(history.history['mean_absolute_error'])\n",
    "        \n",
    "        print(Val_loss_forward[l])\n",
    "        print(val_loss_f[h-1])\n",
    "\n",
    "        Val_loss_forward[l] = val_loss_f[h-1]\n",
    "        Train_loss_forward[l] = train_loss_f[h-1]\n",
    "\n",
    "    ######################################## Backward ###########################    \n",
    "        Train_MV_1_picked = {}\n",
    "        Train_WAV_1_picked = {}\n",
    "\n",
    "        Train_MV_2_picked = {}\n",
    "        Train_WAV_2_picked = {}\n",
    "\n",
    "        Train_MV_3_picked = {}\n",
    "        Train_WAV_3_picked = {}\n",
    "\n",
    "        for i in range(node[2]):        \n",
    "            Train_MV_3_picked[i] = Train_MV_3[i]\n",
    "            Train_WAV_3_picked[i] = Train_WAV_3[i]\n",
    "\n",
    "        for i in range(node[1]): \n",
    "\n",
    "            Train_MV_2_picked[i] = Train_MV_2[i]\n",
    "            Train_WAV_2_picked[i] = Train_WAV_2[i]\n",
    "\n",
    "        for i in range(node[0]):\n",
    "            Train_MV_1_picked[i] = Train_MV_1[i]\n",
    "            Train_WAV_1_picked[i] = Train_WAV_1[i]\n",
    "\n",
    "        Train_MV_picked = list(Train_MV_1_picked.values()) + list(Train_MV_2_picked.values()) + list(Train_MV_3_picked.values()) \n",
    "        Train_WAV_picked = list(Train_WAV_1_picked.values()) + list(Train_WAV_2_picked.values()) + list(Train_WAV_3_picked.values()) \n",
    "\n",
    "        Train_WAV_picked_shuffle, Train_MV_picked_shuffle = shuffle_list(Train_WAV_picked, Train_MV_picked)\n",
    "\n",
    "        Train_MV_block = np.concatenate([Train_MV_picked_shuffle[x] for x in Train_MV_picked_shuffle], 0).astype(np.float32)\n",
    "        Train_WAV_block = np.concatenate([Train_WAV_picked_shuffle[x] for x in Train_WAV_picked_shuffle], 0)\n",
    "\n",
    "        history = model.fit(Train_MV_block, Train_WAV_block, batch_size = 128, epochs = h, validation_data = (Valid_MV_block, Valid_WAV_block), verbose = 0, callbacks = [PrintDot()])\n",
    "        val_loss_b = np.array(history.history['val_mean_absolute_error'])\n",
    "        train_loss_b = np.array(history.history['mean_absolute_error'])\n",
    "\n",
    "        Val_loss_backward[l] = val_loss_b[h-1]\n",
    "        Train_loss_backward[l] = train_loss_b[h-1]\n",
    "\n",
    "        l = l+1\n",
    "        \n",
    "    return Val_loss_forward, Train_loss_forward, Val_loss_backward, Train_loss_backward, nodes, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      ". . [0.]\n",
      "0.3058227\n",
      " \n",
      ". .  \n",
      ". . [0.]\n",
      "0.29919487\n",
      " \n",
      ". .  \n",
      ". . [0.]\n",
      "0.29552588\n",
      " \n",
      ". .  \n",
      ". . . [0.]\n",
      "0.29852775\n",
      " \n",
      ". . .  \n",
      ". . . [0.]\n",
      "0.29405957\n",
      " \n",
      ". . .  \n",
      ". . . [0.]\n",
      "0.28649077\n",
      " \n",
      ". . .  \n",
      ". . . . [0.]\n",
      "0.29195225\n",
      " \n",
      ". . . .  \n",
      ". . . . [0.]\n",
      "0.2833081\n",
      " \n",
      ". . . .  \n",
      ". . . . [0.]\n",
      "0.28526905\n",
      " \n",
      ". . . .  \n",
      ". . . . . [0.]\n",
      "0.2821532\n",
      " \n",
      ". . . . .  \n",
      ". . . . . [0.]\n",
      "0.27910382\n",
      " \n",
      ". . . . .  \n",
      ". . . . . [0.]\n",
      "0.27871794\n",
      " \n",
      ". . . . .  \n",
      ". . . . . . [0.]\n",
      "0.27772647\n",
      " \n",
      ". . . . . .  \n",
      ". . . . . . [0.]\n",
      "0.27693185\n",
      " \n",
      ". . . . . .  \n",
      ". . . . . . [0.]\n",
      "0.27563184\n",
      " \n",
      ". . . . . .  \n",
      ". . . . . . . [0.]\n",
      "0.2757435\n",
      " \n",
      ". . . . . . .  \n",
      ". . . . . . . [0.]\n",
      "0.2735397\n",
      " \n",
      ". . . . . . .  \n",
      ". . . . . . . [0.]\n",
      "0.2728599\n",
      " \n",
      ". . . . . . .  \n",
      ". . . . . . . . [0.]\n",
      "0.27254364\n",
      " \n",
      ". . . . . . . .  \n",
      ". . . . . . . . [0.]\n",
      "0.27541348\n",
      " \n",
      ". . . . . . . .  \n",
      ". . . . . . . . [0.]\n",
      "0.27398077\n",
      " \n",
      ". . . . . . . .  \n",
      ". . . . . . . . . [0.]\n",
      "0.27731106\n",
      " \n",
      ". . . . . . . . .  \n",
      ". . . . . . . . . [0.]\n",
      "0.2780875\n",
      " \n",
      ". . . . . . . . .  \n",
      ". . . . . . . . . [0.]\n",
      "0.2743253\n",
      " \n",
      ". . . . . . . . .  \n",
      ". . . . . . . . . . [0.]\n",
      "0.2802466\n",
      " \n",
      ". . . . . . . . . .  \n",
      ". . . . . . . . . . [0.]\n",
      "0.2737054\n",
      " \n",
      ". . . . . . . . . .  \n",
      ". . . . . . . . . . [0.]\n",
      "0.27931052\n",
      " \n",
      ". . . . . . . . . .  \n",
      ". . . . . . . . . . . [0.]\n",
      "0.28842765\n",
      " \n",
      ". . . . . . . . . . .  \n",
      ". . . . . . . . . . . [0.]\n",
      "0.27888212\n",
      " \n",
      ". . . . . . . . . . .  \n",
      ". . . . . . . . . . . [0.]\n",
      "0.2847704\n",
      " \n",
      ". . . . . . . . . . .  \n",
      ". . . . . . . . . . . . [0.]\n",
      "0.29540172\n",
      " \n",
      ". . . . . . . . . . . .  \n",
      ". . . . . . . . . . . . [0.]\n",
      "0.297473\n",
      " \n",
      ". . . . . . . . . . . .  \n",
      ". . . . . . . . . . . . [0.]\n",
      "0.296124\n",
      " \n",
      ". . . . . . . . . . . .  \n",
      ". . . . . . . . . . . . . [0.]\n",
      "0.29490727\n",
      " \n",
      ". . . . . . . . . . . . .  \n",
      ". . . . . . . . . . . . . [0.]\n",
      "0.30637765\n",
      " \n",
      ". . . . . . . . . . . . .  \n",
      ". . . . . . . . . . . . . [0.]\n",
      "0.29399908\n",
      " \n",
      ". . . . . . . . . . . . .  \n",
      ". . . . . . . . . . . . . . [0.]\n",
      "0.29672852\n",
      " \n",
      ". . . . . . . . . . . . . .  \n",
      ". . . . . . . . . . . . . . [0.]\n",
      "0.29371816\n",
      " \n",
      ". . . . . . . . . . . . . .  \n",
      ". . . . . . . . . . . . . . [0.]\n",
      "0.28817225\n",
      " \n",
      ". . . . . . . . . . . . . .  \n",
      ". . . . . . . . . . . . . . . [0.]\n",
      "0.30134442\n",
      " \n",
      ". . . . . . . . . . . . . . .  \n",
      ". . . . . . . . . . . . . . . [0.]\n",
      "0.29020786\n",
      " \n",
      ". . . . . . . . . . . . . . .  \n",
      ". . . . . . . . . . . . . . . [0.]\n",
      "0.28914776\n",
      " \n",
      ". . . . . . . . . . . . . . . I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". I am working\n",
      " \n",
      ". "
     ]
    }
   ],
   "source": [
    "max_epoch = 300\n",
    "patient = 5\n",
    "\n",
    "not_converge = 0\n",
    "\n",
    "Validation_loss = {}\n",
    "Train_loss = {}\n",
    "\n",
    "epoch_val_loss = {}\n",
    "epoch_val_loss[0] = 100000\n",
    "\n",
    "alpha_init = np.array([0.333, 0.333, 0.333])\n",
    "reduction = 0.5\n",
    "nodes, alphas = node_split(alpha_init, reduction)\n",
    "\n",
    "h = 2\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    \n",
    "    \n",
    "    if not_converge <= patient:\n",
    "    \n",
    "        Val_loss_forward, Train_loss_forward, Val_loss_backward, Train_loss_backward, current_nodes, current_model = valid_nodes(nodes, h)\n",
    "    \n",
    "        total_val_loss = Val_loss_forward + np.flip(Val_loss_backward)\n",
    "    \n",
    "        epoch_val_loss[epoch] = sum(total_val_loss)\n",
    "        \n",
    "        best_node = np.argmin(total_val_loss)\n",
    "        nodes, alphas  = node_split(alphas[best_node], reduction)\n",
    "        \n",
    "        Val_loss = np.concatenate([Val_loss_forward, Val_loss_backward])\n",
    "        Tra_loss = np.concatenate([Train_loss_forward, Train_loss_backward])\n",
    "    \n",
    "        if epoch > 0 and epoch_val_loss[epoch] > epoch_val_loss[epoch -1]:\n",
    "            not_converge = not_converge + 1\n",
    "            \n",
    "        h = h + 1\n",
    "            \n",
    "    \n",
    "    else:\n",
    "        print(\"I am working\")\n",
    "        selected_node = current_nodes[best_node]\n",
    "\n",
    "        Train_MV_1_picked = {}\n",
    "        Train_WAV_1_picked = {}\n",
    "\n",
    "        Train_MV_2_picked = {}\n",
    "        Train_WAV_2_picked = {}\n",
    "\n",
    "        Train_MV_3_picked = {}\n",
    "        Train_WAV_3_picked = {} \n",
    "\n",
    "        for i in range(selected_node[0]):        \n",
    "            Train_MV_1_picked[i] = Train_MV_1[i]\n",
    "            Train_WAV_1_picked[i] = Train_WAV_1[i]\n",
    "\n",
    "        for i in range(selected_node[1]): \n",
    "\n",
    "            Train_MV_2_picked[i] = Train_MV_2[i]\n",
    "            Train_WAV_2_picked[i] = Train_WAV_2[i]\n",
    "\n",
    "        for i in range(selected_node[2]):\n",
    "            Train_MV_3_picked[i] = Train_MV_3[i]\n",
    "            Train_WAV_3_picked[i] = Train_WAV_3[i]\n",
    "\n",
    "        Train_MV_picked = list(Train_MV_1_picked.values()) + list(Train_MV_2_picked.values()) + list(Train_MV_3_picked.values()) \n",
    "        Train_WAV_picked = list(Train_WAV_1_picked.values()) + list(Train_WAV_2_picked.values()) + list(Train_WAV_3_picked.values()) \n",
    "\n",
    "        Train_WAV_picked_shuffle, Train_MV_picked_shuffle = shuffle_list(Train_WAV_picked, Train_MV_picked)\n",
    "\n",
    "        Train_MV_block = np.concatenate([Train_MV_picked_shuffle[x] for x in Train_MV_picked_shuffle], 0).astype(np.float32)\n",
    "        Train_WAV_block = np.concatenate([Train_WAV_picked_shuffle[x] for x in Train_WAV_picked_shuffle], 0)\n",
    "\n",
    "        history = current_model.fit(Train_MV_block, Train_WAV_block, batch_size = 128, epochs = 1, validation_data = (Valid_MV_block, Valid_WAV_block), verbose = 0, callbacks = [PrintDot()])\n",
    "        Val_loss = np.array(history.history['val_mean_absolute_error'])\n",
    "        Tra_loss = np.array(history.history['mean_absolute_error'])\n",
    "       \n",
    "    Validation_loss[epoch] = Val_loss\n",
    "    Train_loss[epoch] = Tra_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTTT_loss = {}\n",
    "VVVV_loss = {}\n",
    "\n",
    "for i in range(len(Validation_loss)):\n",
    "    \n",
    "    TT_loss = Train_loss[i]\n",
    "    TTT_loss = np.reshape(TT_loss, [TT_loss.shape[0] ,1])\n",
    "    TTTT_loss[i] = TTT_loss \n",
    "\n",
    "    VV_loss = Validation_loss[i]\n",
    "    VVV_loss = np.reshape(VV_loss, [VV_loss.shape[0] ,1])\n",
    "    VVVV_loss[i] = VVV_loss\n",
    "    \n",
    "VVVVV_loss = np.concatenate([VVVV_loss[x] for x in VVVV_loss])\n",
    "TTTTT_loss = np.concatenate([TTTT_loss[x] for x in TTTT_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f377c0eca20>]"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(TTTTT_loss, label = 'Train Loss')\n",
    "plt.plot(VVVVV_loss, label = 'Valid Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 14   1 227]\n"
     ]
    }
   ],
   "source": [
    "print(current_nodes[best_node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244\n"
     ]
    }
   ],
   "source": [
    "print(len(Train_MV_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(Test_MV_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.861459603456279\n"
     ]
    }
   ],
   "source": [
    "mgc_head = test_predictions[:, 0:mgc_dim]\n",
    "mgc_ref = Test_WAV_block[:, 0:mgc_dim]\n",
    "from nnmnkwii.metrics import melcd\n",
    "\n",
    "MCD = melcd(mgc_head[:,1:mgc_dim], mgc_ref[:,1:mgc_dim], lengths=None)\n",
    "print(MCD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
